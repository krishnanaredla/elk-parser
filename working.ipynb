{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LineId      Date      Time Level                              Component  \\\n",
      "0       1  17/06/20  05:10:40  INFO  executor.CoarseGrainedExecutorBackend   \n",
      "1       2  17/06/20  05:10:40  INFO                  spark.SecurityManager   \n",
      "2       3  17/06/20  05:10:40  INFO                        com.org.sravani   \n",
      "3       4  17/06/20  05:10:40  INFO                  spark.SecurityManager   \n",
      "4       5  17/06/20  05:10:40  INFO                  spark.SecurityManager   \n",
      "\n",
      "                                             Content   EventId  \\\n",
      "0    Registered signal handlers for [TERM, HUP, INT]  932c104e   \n",
      "1                   Changing view acls to: yarn,curi  7bb9d001   \n",
      "2                                  Finishing the job  93322a10   \n",
      "3                 Changing modify acls to: yarn,curi  7bb9d001   \n",
      "4   SecurityManager: authentication disabled; ui ...  29bc3dfc   \n",
      "\n",
      "                                       EventTemplate ParameterList  \n",
      "0    Registered signal handlers for [TERM, HUP, INT]            []  \n",
      "1                    Changing <*> acls to: yarn,curi            []  \n",
      "2                                  Finishing the job            []  \n",
      "3                    Changing <*> acls to: yarn,curi            []  \n",
      "4  SecurityManager: authentication disabled; ui a...            []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"result/Spark_2k.log_structured.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import parallel_bulk\n",
    "from collections import deque\n",
    "from typing import Any, Dict, Generator, List, Tuple, Union\n",
    "from elasticsearch import Elasticsearch\n",
    "import uuid\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 10000\n",
    "DEFAULT_THREAD_COUNT = 4\n",
    "\n",
    "def action_generator(\n",
    "    pd_df: pd.DataFrame,\n",
    "    es_dest_index: str,\n",
    ") -> Generator[Dict[str, Any], None, None]:\n",
    "    for row in pd_df.iterrows():\n",
    "        values = row[1].to_dict()\n",
    "        id = row[0]\n",
    "        #action = {\"_index\": es_dest_index, \"_source\": values, \"_id\": str(id)}\n",
    "        action = {\n",
    "            \"_op_type\": \"update\",\n",
    "            \"_index\": es_dest_index,\n",
    "            \"_id\": str(uuid.uuid4()),\n",
    "            \"_source\": {\n",
    "                \"doc\": values,\n",
    "                \"doc_as_upsert\": True\n",
    "            }\n",
    "        }\n",
    "        yield action\n",
    "\n",
    "def pushData(es_client:Union[str, List[str], Tuple[str, ...], Elasticsearch],pd_df: pd.DataFrame,es_dest_index:str) :\n",
    "    try:\n",
    "        deque(\n",
    "        parallel_bulk(\n",
    "            client=es_client,\n",
    "            actions=action_generator(\n",
    "                pd_df, es_dest_index\n",
    "            ),\n",
    "            chunk_size=int(DEFAULT_CHUNK_SIZE / DEFAULT_THREAD_COUNT),\n",
    "        ),\n",
    "        maxlen=0,\n",
    "    )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client=Elasticsearch(hosts='http://localhost:9200', http_auth=('elastic', 'mangena'))\n",
    "es_index = \"logs-pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushData(es_client,df,es_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/06/2305:10:40\n"
     ]
    }
   ],
   "source": [
    "for row in df.iterrows(): \n",
    "        print(str(row[1][\"LineId\"])+row[1][\"Date\"]+row[1][\"Time\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
